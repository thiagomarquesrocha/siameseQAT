{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distance(feature, squared=False):\n",
    "    \"\"\"Computes the pairwise distance matrix with numerical stability.\n",
    "\n",
    "    output[i, j] = || feature[i, :] - feature[j, :] ||_2\n",
    "\n",
    "    Args:\n",
    "      feature: 2-D Tensor of size [number of data, feature dimension].\n",
    "      squared: Boolean, whether or not to square the pairwise distances.\n",
    "\n",
    "    Returns:\n",
    "      pairwise_distances: 2-D Tensor of size [number of data, number of data].\n",
    "    \"\"\"\n",
    "    pairwise_distances_squared = math_ops.add(\n",
    "        math_ops.reduce_sum(math_ops.square(feature), axis=[1], keepdims=True),\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.square(array_ops.transpose(feature)),\n",
    "            axis=[0],\n",
    "            keepdims=True)) - 2.0 * math_ops.matmul(feature,\n",
    "                                                    array_ops.transpose(feature))\n",
    "\n",
    "    # Deal with numerical inaccuracies. Set small negatives to zero.\n",
    "    pairwise_distances_squared = math_ops.maximum(pairwise_distances_squared, 0.0)\n",
    "    # Get the mask where the zero distances are at.\n",
    "    error_mask = math_ops.less_equal(pairwise_distances_squared, 0.0)\n",
    "\n",
    "    # Optionally take the sqrt.\n",
    "    if squared:\n",
    "        pairwise_distances = pairwise_distances_squared\n",
    "    else:\n",
    "        pairwise_distances = math_ops.sqrt(\n",
    "            pairwise_distances_squared + math_ops.to_float(error_mask) * 1e-16)\n",
    "\n",
    "    # Undo conditionally adding 1e-16.\n",
    "    pairwise_distances = math_ops.multiply(\n",
    "        pairwise_distances, math_ops.to_float(math_ops.logical_not(error_mask)))\n",
    "\n",
    "    num_data = array_ops.shape(feature)[0]\n",
    "    # Explicitly set diagonals to zero.\n",
    "    mask_offdiagonals = array_ops.ones_like(pairwise_distances) - array_ops.diag(\n",
    "        array_ops.ones([num_data]))\n",
    "    pairwise_distances = math_ops.multiply(pairwise_distances, mask_offdiagonals)\n",
    "    return pairwise_distances\n",
    "\n",
    "def masked_maximum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise maximum over chosen elements.\n",
    "\n",
    "    Args:\n",
    "      data: 2-D float `Tensor` of size [n, m].\n",
    "      mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "      dim: The dimension over which to compute the maximum.\n",
    "\n",
    "    Returns:\n",
    "      masked_maximums: N-D `Tensor`.\n",
    "        The maximized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_minimums = math_ops.reduce_min(data, dim, keepdims=True)\n",
    "    masked_maximums = math_ops.reduce_max(\n",
    "        math_ops.multiply(data - axis_minimums, mask), dim,\n",
    "        keepdims=True) + axis_minimums\n",
    "    return masked_maximums\n",
    "\n",
    "def masked_minimum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise minimum over chosen elements.\n",
    "\n",
    "    Args:\n",
    "      data: 2-D float `Tensor` of size [n, m].\n",
    "      mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "      dim: The dimension over which to compute the minimum.\n",
    "\n",
    "    Returns:\n",
    "      masked_minimums: N-D `Tensor`.\n",
    "        The minimized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_maximums = math_ops.reduce_max(data, dim, keepdims=True)\n",
    "    masked_minimums = math_ops.reduce_min(\n",
    "        math_ops.multiply(data - axis_maximums, mask), dim,\n",
    "        keepdims=True) + axis_maximums\n",
    "    return masked_minimums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quintet Loss with Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24885336,\n",
       " 2.5184083,\n",
       " 0.25,\n",
       " 0.84415996,\n",
       " 0.52272725,\n",
       " 0.71148455,\n",
       " 0.045454547,\n",
       " 0.9627639)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## required for semi-hard triplet loss:\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "\n",
    "def triplet_loss(vects):\n",
    "    margin = 1.\n",
    "    labels = vects[:, :1]\n",
    " \n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "\n",
    "    embeddings = vects[:, 1:]\n",
    "\n",
    "    ### Code from Tensorflow function [tf.contrib.losses.metric_learning.triplet_semihard_loss] starts here:\n",
    "    \n",
    "    # Reshape [batch_size] label tensor to a [batch_size, 1] label tensor.\n",
    "    # lshape=array_ops.shape(labels)\n",
    "    # assert lshape.shape == 1\n",
    "    # labels = array_ops.reshape(labels, [lshape[0], 1])\n",
    "\n",
    "    # Build pairwise squared distance matrix.\n",
    "    pdist_matrix = pairwise_distance(embeddings, squared=True)\n",
    "    # Build pairwise binary adjacency matrix.\n",
    "    adjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
    "    # Invert so we can select negatives only.\n",
    "    adjacency_not = math_ops.logical_not(adjacency)\n",
    "\n",
    "    # global batch_size  \n",
    "    batch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
    "\n",
    "    # Compute the mask.\n",
    "    pdist_matrix_tile = array_ops.tile(pdist_matrix, [batch_size, 1])\n",
    "    mask = math_ops.logical_and(\n",
    "        array_ops.tile(adjacency_not, [batch_size, 1]),\n",
    "        math_ops.greater(\n",
    "            pdist_matrix_tile, array_ops.reshape(\n",
    "                array_ops.transpose(pdist_matrix), [-1, 1])))\n",
    "    mask_final = array_ops.reshape(\n",
    "        math_ops.greater(\n",
    "            math_ops.reduce_sum(\n",
    "                math_ops.cast(mask, dtype=dtypes.float32), 1, keepdims=True),\n",
    "            0.0), [batch_size, batch_size])\n",
    "    mask_final = array_ops.transpose(mask_final)\n",
    "\n",
    "    adjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
    "    mask = math_ops.cast(mask, dtype=dtypes.float32)\n",
    "\n",
    "    # negatives_outside: smallest D_an where D_an > D_ap.\n",
    "    negatives_outside = array_ops.reshape(\n",
    "        masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n",
    "    negatives_outside = array_ops.transpose(negatives_outside)\n",
    "\n",
    "    # negatives_inside: largest D_an.\n",
    "    negatives_inside = array_ops.tile(\n",
    "        masked_maximum(pdist_matrix, adjacency_not), [1, batch_size])\n",
    "    semi_hard_negatives = array_ops.where(\n",
    "        mask_final, negatives_outside, negatives_inside)\n",
    "\n",
    "    loss_mat = math_ops.add(margin, pdist_matrix - semi_hard_negatives)\n",
    "\n",
    "    mask_positives = math_ops.cast(\n",
    "        adjacency, dtype=dtypes.float32) - array_ops.diag(\n",
    "        array_ops.ones([batch_size]))\n",
    "\n",
    "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "    #   in semihard, they take all positive pairs except the diagonal.\n",
    "    num_positives = math_ops.reduce_sum(mask_positives)\n",
    "\n",
    "    semi_hard_triplet_loss_distance = math_ops.truediv(\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.maximum(\n",
    "                math_ops.multiply(loss_mat, mask_positives), 0.0)),\n",
    "        num_positives,\n",
    "        name='triplet_semihard_loss')\n",
    "    \n",
    "    ### Code from Tensorflow function semi-hard triplet loss ENDS here.\n",
    "    return semi_hard_triplet_loss_distance\n",
    "\n",
    "def quintet_loss(vects):\n",
    "    margin = 1.\n",
    "    labels = vects[:, :1]\n",
    " \n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "\n",
    "    embeddings = vects[:, 1:]\n",
    "\n",
    "    # Build pairwise squared distance matrix.\n",
    "    pdist_matrix = pairwise_distance(embeddings, squared=True)\n",
    "    # Build pairwise binary adjacency matrix.\n",
    "    adjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
    "    # Invert so we can select negatives only.\n",
    "    adjacency_not = math_ops.logical_not(adjacency)\n",
    "\n",
    "    # global batch_size  \n",
    "    batch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
    "\n",
    "    adjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
    "\n",
    "    mask_positives = math_ops.cast(\n",
    "        adjacency, dtype=dtypes.float32) - array_ops.diag(array_ops.ones([batch_size]))\n",
    "\n",
    "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "    #   in semihard, they take all positive pairs except the diagonal.\n",
    "    num_positives = math_ops.reduce_sum(mask_positives)\n",
    "    \n",
    "    mask_negatives = adjacency_not\n",
    "    \n",
    "    # pos \n",
    "    embed_pos = tf.matmul(mask_positives, embeddings)\n",
    "    num_of_pos = tf.reduce_sum(mask_positives, axis=1, keepdims=True)\n",
    "    centroid_embed_pos = tf.math.xdivy(embed_pos, num_of_pos)\n",
    "    \n",
    "    # add centroids to the batch\n",
    "    embeddings_anchor_centroid_pos = tf.concat([embeddings, centroid_embed_pos], axis=0)\n",
    "    \n",
    "    # add label for centroids\n",
    "    labels_pos = tf.concat([labels, labels], axis=0)\n",
    "    labels_pos = tf.cast(labels_pos, dtype=dtypes.float32)\n",
    "    vects_pos = tf.concat([tf.reshape(labels_pos, (-1, 1)), embeddings_anchor_centroid_pos], axis=1)\n",
    "    return vects_pos\n",
    "\n",
    "    # neg\n",
    "    #return mask_negatives\n",
    "    embed_neg = tf.matmul(mask_negatives, embeddings)\n",
    "    num_of_neg = tf.reduce_sum(mask_negatives, axis=1, keepdims=True)\n",
    "    centroid_embed_neg = tf.math.xdivy(embed_neg, num_of_neg)\n",
    "    # add centroids to the batch\n",
    "    embeddings_anchor_centroid_neg = tf.concat([embeddings, centroid_embed_neg], axis=0)\n",
    "    # create the matrix of neg ids\n",
    "    repeat = tf.fill([1, batch_size], 1)[0]\n",
    "    neg_ids = tf.repeat(tf.reshape(labels, (1, -1)), repeats=[batch_size], axis=0)\n",
    "    mask_negatives_bool = tf.cast(mask_negatives, dtype=dtypes.bool)\n",
    "    neg_ids = tf.cast(neg_ids, dtype=dtypes.float32)\n",
    "    # TODO: get the most frequent neg id\n",
    "#     return tf.where(mask_negatives_bool, neg_ids, mask_negatives)\n",
    "#     unique, _, count = tf.unique_with_counts(tf.where(mask_negatives_bool, neg_ids, mask_negatives))\n",
    "#     max_occurrences = tf.reduce_max(count)\n",
    "#     max_cond = tf.equal(count, max_occurrences)\n",
    "    #max_numbers = tf.squeeze(tf.gather(unique, tf.where(max_cond)))\n",
    "    #return max_cond\n",
    "    neg_ids = tf.reduce_max(tf.where(mask_negatives_bool, neg_ids, mask_negatives), axis=1, keepdims=True)\n",
    "    #return neg_ids\n",
    "    labels_neg = tf.concat([tf.cast(labels, dtype=dtypes.float32), neg_ids], axis=0)\n",
    "    #return labels_neg\n",
    "    vects_neg = tf.concat([tf.reshape(labels_neg, (-1, 1)), embeddings_anchor_centroid_pos], axis=1)\n",
    "#     return vects_neg\n",
    "    \n",
    "    #return triplet_loss(vects), triplet_loss(vects_pos), triplet_loss(vects_neg)\n",
    "    #return triplet_loss(vects)\n",
    "    return tf.reduce_mean([triplet_loss(vects), triplet_loss(vects_pos) * 0.5, triplet_loss(vects_neg) * 0.5])\n",
    "    \n",
    "\n",
    "v = tf.constant([[1.0, 1.0, 2.0], [2.0, 3.0, 4.0], [1.0, 1.0, 2.0], \n",
    "                 [1.0, 1.0, 2.0], [3.0, 3.0, 4.0], [3.0, 1.0, 2.0]]) # [2, 3.0, 4.0] [1, 1.0, 2.0]\n",
    "tl_loss = quintet_loss(v)\n",
    "sess=tf.Session() \n",
    "sess.run(tl_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(0.52771944, 1.0, 0.25, 0.018982518, 0.52272725, 0.8764739, 0.62, 0.10454356)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Thiago\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-c258f5602f84>\", line 2, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"C:\\Users\\Thiago\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\n",
      "    from tensorflow_core import *\n",
      "  File \"C:\\Users\\Thiago\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\n",
      "  File \"C:\\Users\\Thiago\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\Thiago\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\Thiago\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\Thiago\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow_core\\python\\__init__.py\", line 95, in <module>\n",
      "    from tensorflow.python import keras\n",
      "  File \"C:\\Users\\Thiago\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow_core\\python\\keras\\__init__.py\", line 27, in <module>\n",
      "    from tensorflow.python.keras import models\n",
      "  File \"C:\\Users\\Thiago\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow_core\\python\\keras\\__init__.py\", line 27, in <module>\n",
      "    from tensorflow.python.keras import models\n",
      "  File \"C:\\Users\\Thiago\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow_core\\python\\keras\\models.py\", line 26, in <module>\n",
      "    from tensorflow.python.keras.engine import sequential\n",
      "  File \"C:\\Users\\Thiago\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow_core\\python\\keras\\engine\\sequential.py\", line 28, in <module>\n",
      "    from tensorflow.python.keras.engine import training\n",
      "  File \"C:\\Users\\Thiago\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 51, in <module>\n",
      "    from tensorflow.python.keras.engine import training_v2\n",
      "  File \"C:\\Users\\Thiago\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 35, in <module>\n",
      "    from tensorflow.python.keras.engine import data_adapter\n",
      "  File \"C:\\Users\\Thiago\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\", line 50, in <module>\n",
      "    import pandas as pd  # pylint: disable=g-import-not-at-top\n",
      "  File \"C:\\Users\\Thiago\\Anaconda3\\lib\\site-packages\\pandas\\__init__.py\", line 26, in <module>\n",
      "    from pandas._libs import (hashtable as _hashtable,\n",
      "  File \"C:\\Users\\Thiago\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\__init__.py\", line 4, in <module>\n",
      "    from .tslibs import (\n",
      "  File \"C:\\Users\\Thiago\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\tslibs\\__init__.py\", line 4, in <module>\n",
      "    from .conversion import normalize_date, localize_pydatetime, tz_convert_single\n",
      "  File \"pandas\\_libs\\tslibs\\conversion.pyx\", line 1, in init pandas._libs.tslibs.conversion\n",
      "  File \"pandas\\_libs\\tslibs\\timedeltas.pyx\", line 1, in init pandas._libs.tslibs.timedeltas\n",
      "  File \"pandas\\_libs\\tslibs\\offsets.pyx\", line 1, in init pandas._libs.tslibs.offsets\n",
      "  File \"pandas\\_libs\\tslibs\\ccalendar.pyx\", line 12, in init pandas._libs.tslibs.ccalendar\n",
      "  File \"pandas\\_libs\\tslibs\\strptime.pyx\", line 591, in init pandas._libs.tslibs.strptime\n",
      "  File \"pandas\\_libs\\tslibs\\strptime.pyx\", line 534, in pandas._libs.tslibs.strptime.TimeRE.__init__\n",
      "  File \"pandas\\_libs\\tslibs\\strptime.pyx\", line 550, in pandas._libs.tslibs.strptime.TimeRE.__seqToRE\n",
      "  File \"C:\\Users\\Thiago\\Anaconda3\\lib\\site-packages\\pytz\\lazy.py\", line 101, in _lazy\n",
      "    list.extend(self, fill_iter.pop())\n",
      "  File \"C:\\Users\\Thiago\\Anaconda3\\lib\\site-packages\\pytz\\__init__.py\", line 1080, in <genexpr>\n",
      "    tz for tz in all_timezones if resource_exists(tz))\n",
      "  File \"C:\\Users\\Thiago\\Anaconda3\\lib\\site-packages\\pytz\\__init__.py\", line 113, in resource_exists\n",
      "    open_resource(name).close()\n",
      "  File \"C:\\Users\\Thiago\\Anaconda3\\lib\\site-packages\\pytz\\__init__.py\", line 107, in open_resource\n",
      "    return open(filename, 'rb')\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Thiago\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 81, in _path_is_mode_type\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 75, in _path_stat\n",
      "FileNotFoundError: [WinError 2] O sistema n√£o pode encontrar o arquivo especificado: 'C:\\\\Users\\\\Thiago\\\\AppData\\\\Roaming\\\\Python\\\\Python35\\\\site-packages\\\\tensorflow_core\\\\_api\\\\v2\\\\compat\\\\v1\\\\compat\\\\v1\\\\__init__.cp35-win_amd64.pyd'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Thiago\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Thiago\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Thiago\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Thiago\\Anaconda3\\lib\\inspect.py\", line 1453, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Thiago\\Anaconda3\\lib\\inspect.py\", line 1414, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"C:\\Users\\Thiago\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 165, in findsource\n",
      "    file = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\Users\\Thiago\\Anaconda3\\lib\\inspect.py\", line 672, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\Thiago\\Anaconda3\\lib\\inspect.py\", line 709, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\Thiago\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\Thiago\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\Thiago\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 944, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 673, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 665, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Thiago\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow_core\\__init__.py\", line 46, in <module>\n",
      "    from . _api.v2 import compat\n",
      "  File \"C:\\Users\\Thiago\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow_core\\_api\\v2\\compat\\__init__.py\", line 39, in <module>\n",
      "    from . import v1\n",
      "  File \"C:\\Users\\Thiago\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\__init__.py\", line 32, in <module>\n",
      "    from . import compat\n",
      "  File \"C:\\Users\\Thiago\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\compat\\__init__.py\", line 39, in <module>\n",
      "    from . import v1\n",
      "  File \"C:\\Users\\Thiago\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\compat\\v1\\__init__.py\", line 29, in <module>\n",
      "    from tensorflow._api.v2.compat.v1 import app\n",
      "  File \"C:\\Users\\Thiago\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow_core\\_api\\v2\\compat\\__init__.py\", line 39, in <module>\n",
      "    from . import v1\n",
      "  File \"C:\\Users\\Thiago\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\__init__.py\", line 32, in <module>\n",
      "    from . import compat\n",
      "  File \"C:\\Users\\Thiago\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\compat\\__init__.py\", line 39, in <module>\n",
      "    from . import v1\n",
      "  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 954, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 896, in _find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1139, in find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1113, in _get_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1240, in find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 89, in _path_isfile\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 81, in _path_is_mode_type\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "## required for semi-hard triplet loss:\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "import numpy as np\n",
    "\n",
    "def triplet_loss(vects):\n",
    "    margin = 1.\n",
    "    labels = vects[:, :1]\n",
    " \n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "\n",
    "    embeddings = vects[:, 1:]\n",
    "\n",
    "    ### Code from Tensorflow function [tf.contrib.losses.metric_learning.triplet_semihard_loss] starts here:\n",
    "    \n",
    "    # Reshape [batch_size] label tensor to a [batch_size, 1] label tensor.\n",
    "    # lshape=array_ops.shape(labels)\n",
    "    # assert lshape.shape == 1\n",
    "    # labels = array_ops.reshape(labels, [lshape[0], 1])\n",
    "\n",
    "    # Build pairwise squared distance matrix.\n",
    "    pdist_matrix = pairwise_distance(embeddings, squared=True)\n",
    "    # Build pairwise binary adjacency matrix.\n",
    "    adjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
    "    # Invert so we can select negatives only.\n",
    "    adjacency_not = math_ops.logical_not(adjacency)\n",
    "\n",
    "    # global batch_size  \n",
    "    batch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
    "\n",
    "    # Compute the mask.\n",
    "    pdist_matrix_tile = array_ops.tile(pdist_matrix, [batch_size, 1])\n",
    "    mask = math_ops.logical_and(\n",
    "        array_ops.tile(adjacency_not, [batch_size, 1]),\n",
    "        math_ops.greater(\n",
    "            pdist_matrix_tile, array_ops.reshape(\n",
    "                array_ops.transpose(pdist_matrix), [-1, 1])))\n",
    "    mask_final = array_ops.reshape(\n",
    "        math_ops.greater(\n",
    "            math_ops.reduce_sum(\n",
    "                math_ops.cast(mask, dtype=dtypes.float32), 1, keepdims=True),\n",
    "            0.0), [batch_size, batch_size])\n",
    "    mask_final = array_ops.transpose(mask_final)\n",
    "\n",
    "    adjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
    "    mask = math_ops.cast(mask, dtype=dtypes.float32)\n",
    "\n",
    "    # negatives_outside: smallest D_an where D_an > D_ap.\n",
    "    negatives_outside = array_ops.reshape(\n",
    "        masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n",
    "    negatives_outside = array_ops.transpose(negatives_outside)\n",
    "\n",
    "    # negatives_inside: largest D_an.\n",
    "    negatives_inside = array_ops.tile(\n",
    "        masked_maximum(pdist_matrix, adjacency_not), [1, batch_size])\n",
    "    semi_hard_negatives = array_ops.where(\n",
    "        mask_final, negatives_outside, negatives_inside)\n",
    "\n",
    "    loss_mat = math_ops.add(margin, pdist_matrix - semi_hard_negatives)\n",
    "\n",
    "    mask_positives = math_ops.cast(\n",
    "        adjacency, dtype=dtypes.float32) - array_ops.diag(\n",
    "        array_ops.ones([batch_size]))\n",
    "\n",
    "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "    #   in semihard, they take all positive pairs except the diagonal.\n",
    "    num_positives = math_ops.reduce_sum(mask_positives)\n",
    "\n",
    "    semi_hard_triplet_loss_distance = math_ops.truediv(\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.maximum(\n",
    "                math_ops.multiply(loss_mat, mask_positives), 0.0)),\n",
    "        num_positives,\n",
    "        name='triplet_semihard_loss')\n",
    "    \n",
    "    ### Code from Tensorflow function semi-hard triplet loss ENDS here.\n",
    "    return semi_hard_triplet_loss_distance\n",
    "\n",
    "def quintet_loss(inputs):\n",
    "    margin = 1.\n",
    "    labels = inputs[:, :1]\n",
    " \n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "\n",
    "    embeddings = inputs[:, 1:]\n",
    "\n",
    "    # Build pairwise squared distance matrix.\n",
    "    pdist_matrix = pairwise_distance(embeddings, squared=True)\n",
    "    # Build pairwise binary adjacency matrix.\n",
    "    adjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
    "    # Invert so we can select negatives only.\n",
    "    adjacency_not = math_ops.logical_not(adjacency)\n",
    "\n",
    "    # global batch_size  \n",
    "    batch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
    "\n",
    "    adjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
    "\n",
    "    mask_positives = math_ops.cast(\n",
    "        adjacency, dtype=dtypes.float32) - array_ops.diag(array_ops.ones([batch_size]))\n",
    "\n",
    "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "    #   in semihard, they take all positive pairs except the diagonal.\n",
    "    num_positives = math_ops.reduce_sum(mask_positives)\n",
    "    \n",
    "    mask_negatives = adjacency_not\n",
    "    \n",
    "    # Include the anchor to positives\n",
    "#     mask_positives = math_ops.cast(adjacency, dtype=dtypes.float32)\n",
    "    \n",
    "#     return mask_positives\n",
    "    \n",
    "    # pos \n",
    "    embed_pos = tf.matmul(mask_positives, embeddings)\n",
    "    num_of_pos = tf.reduce_sum(mask_positives, axis=1, keepdims=True)\n",
    "    centroid_embed_pos = tf.math.xdivy(embed_pos, num_of_pos)\n",
    "    \n",
    "    # add centroids to the batch\n",
    "    embeddings_anchor_centroid_pos = tf.concat([embeddings, centroid_embed_pos], axis=0)\n",
    "    \n",
    "    # add label for centroids\n",
    "    labels_pos = tf.concat([labels, labels], axis=0)\n",
    "    labels_pos = tf.cast(labels_pos, dtype=dtypes.float32)\n",
    "    vects_pos = tf.concat([tf.reshape(labels_pos, (-1, 1)), embeddings_anchor_centroid_pos], axis=1)\n",
    "    return mask_positives\n",
    "\n",
    "    # neg\n",
    "    # create the matrix of neg ids\n",
    "    repeat = tf.fill([1, batch_size], 1)[0]\n",
    "    neg_ids = tf.repeat(tf.reshape(labels, (1, -1)), repeats=[batch_size], axis=0)\n",
    "    mask_negatives_bool = tf.cast(mask_negatives, dtype=dtypes.bool)\n",
    "    neg_ids = tf.cast(neg_ids, dtype=dtypes.float32)\n",
    "    \n",
    "    i = tf.constant(0)\n",
    "    most_freq_matrix = tf.Variable([])\n",
    "    neg_matrix=tf.where(mask_negatives_bool, neg_ids, mask_negatives)\n",
    "#     return neg_matrix\n",
    "    def most_frequent(i, most_freq_matrix):\n",
    "        batch = tf.gather(neg_matrix, i)\n",
    "        neg_label_default = [tf.unique(batch)[0][0]]\n",
    "        batch = tf.boolean_mask(batch, tf.greater(batch, 0))\n",
    "        unique, _, count = tf.unique_with_counts(batch)\n",
    "        max_occurrences = tf.reduce_max(count)\n",
    "        max_cond = tf.equal(count, max_occurrences)\n",
    "        max_numbers = tf.squeeze(tf.gather(unique, tf.where(max_cond)))\n",
    "        max_numbers = tf.cond(tf.cast(tf.size(unique) > 1, tf.bool), lambda: unique[0], lambda: max_numbers)\n",
    "        max_numbers = tf.cond(tf.cast(tf.shape(unique) == 0, tf.bool), \n",
    "                              lambda: neg_label_default, \n",
    "                              lambda: max_numbers)\n",
    "        most_freq_matrix = tf.concat([most_freq_matrix, [max_numbers]], axis=0)\n",
    "        return [tf.add(i, 1), most_freq_matrix]\n",
    "#     _, most_freq_matrix = tf.while_loop(lambda i, _: i<batch_size, \n",
    "#                                         most_frequent, \n",
    "#                                         [i, most_freq_matrix],\n",
    "#                                        shape_invariants=[i.get_shape(),\n",
    "#                                                    tf.TensorShape([None])])\n",
    "    \n",
    "    neg_random_matrix = tf.Variable([])\n",
    "    def random_negative(i, neg_random_matrix):\n",
    "        batch = tf.gather(neg_matrix, i)\n",
    "        batch = tf.boolean_mask(batch, tf.greater(batch, 0))\n",
    "        prob = tf.random_uniform_initializer(minval=0., maxval=1.)(shape=[1])[0]\n",
    "        value = tf.cast(prob * tf.cast(tf.size(batch), dtype=dtypes.float32), dtype=dtypes.int32)\n",
    "        value = batch[value]\n",
    "        neg_random_matrix = tf.concat([neg_random_matrix, [value]], axis=0)\n",
    "        return [tf.add(i, 1), neg_random_matrix]\n",
    "    _, neg_random_matrix = tf.while_loop(lambda i, _: i<batch_size, \n",
    "                                        random_negative, \n",
    "                                        [i, neg_random_matrix],\n",
    "                                       shape_invariants=[i.get_shape(),\n",
    "                                                   tf.TensorShape([None])])\n",
    "    \n",
    "    new_neg_ids = tf.reshape(neg_random_matrix, (batch_size, 1))\n",
    "    mask_negatives = tf.cast(tf.equal(neg_ids, new_neg_ids), dtype=dtypes.float32)\n",
    "#     return mask_negatives\n",
    "    \n",
    "    neg_ids = new_neg_ids\n",
    "    \n",
    "    # Embedding negs\n",
    "    embed_neg = tf.matmul(mask_negatives, embeddings)\n",
    "    num_of_neg = tf.reduce_sum(mask_negatives, axis=1, keepdims=True)\n",
    "    centroid_embed_neg = tf.math.xdivy(embed_neg, num_of_neg)\n",
    "    # add centroids to the batch\n",
    "    embeddings_anchor_centroid_neg = tf.concat([embeddings, centroid_embed_neg], axis=0)\n",
    "#     return neg_ids\n",
    "#     return tf.where(mask_negatives_bool, neg_ids, mask_negatives)\n",
    "    # Get the biggest id\n",
    "#     neg_ids = tf.reduce_max(tf.where(mask_negatives_bool, neg_ids, mask_negatives), axis=1, keepdims=True)\n",
    "#     return neg_ids\n",
    "    labels_neg = neg_ids\n",
    "    labels_neg = tf.concat([tf.cast(labels, dtype=dtypes.float32), neg_ids], axis=0)\n",
    "#     return labels_neg\n",
    "#     return embeddings_anchor_centroid_neg\n",
    "    vects_neg = tf.concat([labels_neg, embeddings_anchor_centroid_neg], axis=1)\n",
    "#     return vects_neg\n",
    "    \n",
    "    # Centroids embed\n",
    "#     return triplet_loss(vects)\n",
    "    embeddings = tf.concat([embeddings, centroid_embed_pos, centroid_embed_neg], axis=0)\n",
    "    labels = tf.cast(labels, dtype=dtypes.float32)\n",
    "    labels = tf.concat([labels, labels, neg_ids], axis=0)\n",
    "    vects_centroids =  tf.concat([tf.reshape(labels, (-1, 1)), embeddings], axis=1)\n",
    "    #return triplet_loss(vects), triplet_loss(vects_pos), triplet_loss(vects_neg)\n",
    "    #return triplet_loss(vects)\n",
    "    \n",
    "#     return TL_w\n",
    "    TL_anchor_w = tf.random_uniform_initializer(minval=0.0, maxval=1.)(shape=[1])[0]\n",
    "    TL_centroid_w = tf.random_uniform_initializer(minval=0.0, maxval=1.)(shape=[1])[0]\n",
    "    TL_pos_w = tf.random_uniform_initializer(minval=0.0, maxval=1.)(shape=[1])[0]\n",
    "    TL_neg_w = tf.random_uniform_initializer(minval=0.0, maxval=1.)(shape=[1])[0]\n",
    "#     TL_centroid_w = TL_w[0][3]\n",
    "    #sum_of_w = tf.reduce_sum(TL_w)\n",
    "    #TL_w = tf.truediv(TL_w, sum_of_w)\n",
    "    # Normalization in probabilities\n",
    "    #TL_anchor_w = tf.truediv(TL_anchor_w, sum_of_w)\n",
    "    #TL_pos_w = tf.truediv(TL_pos_w, sum_of_w)\n",
    "    #TL_neg_w = tf.truediv(TL_neg_w, sum_of_w)\n",
    "#     return TL_pos_w, TL_neg_w\n",
    "    TL = triplet_loss(inputs)\n",
    "    TL_pos = triplet_loss(vects_pos)\n",
    "    TL_neg = triplet_loss(vects_neg)\n",
    "    TL_centroid = triplet_loss(vects_centroids)\n",
    "#     TL_centroid = 0.0\n",
    "    sum_of_median = tf.reduce_sum([TL * TL_anchor_w, TL_pos * TL_pos_w, TL_neg * TL_neg_w, TL_centroid * TL_centroid_w])  \n",
    "    sum_of_weigths = TL_anchor_w + TL_pos_w + TL_neg_w + TL_centroid_w \n",
    "    weigthed_median = tf.truediv(sum_of_median, sum_of_weigths)    \n",
    "    return tf.cast([weigthed_median, TL_anchor_w, TL_pos_w, TL_neg_w, TL_centroid_w, TL, TL_pos, TL_neg, TL_centroid], \n",
    "                   dtype=dtypes.float32)\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[0])\n",
    "\n",
    "def TL_w_anchor(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[1])\n",
    "def TL_w_pos(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[2])\n",
    "def TL_w_neg(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[3])\n",
    "def TL_w_centroid(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[4])\n",
    "def TL(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[5])\n",
    "def TL_pos(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[6])\n",
    "def TL_neg(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[7])\n",
    "def TL_centroid(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred[8])\n",
    "\n",
    "v = tf.constant([[1.0, 1.0, 2.0], [2.0, 3.0, 4.0], [1.0, 1.0, 2.0], \n",
    "                 [1.0, 1.0, 2.0], [3.0, 3.0, 4.0], [3.0, 1.0, 2.0]]) # [2, 3.0, 4.0] [1, 1.0, 2.0]\n",
    "tl_loss = quintet_loss(v)\n",
    "sess=tf.Session() \n",
    "sess.run(tl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [130926,133860,284787,102727,275267,352297,159354,295390,381207,321088\n",
    ",351176,411703,250243,46067,190119,125376,424120,298717,291029,152442\n",
    ",301724,36328,183961,416809,3407,144150,329419,374624,269326,410734\n",
    ",374624,65934,220763,14679,13556,301593,184909,295390,76922,40227\n",
    ",169079,31064,2859,200295,374624,153304,374624,159354,332879,257318\n",
    ",424120,30610,350370,236970,168880,3407,47818,128387,188385,111626\n",
    ",374624,58431,13556,344913,194533,198282,117300,104540,301724,198282\n",
    ",240969,240969,187251,102727,231616,2644,95462,240969,168880,3407\n",
    ",295390,86523,286212,301724,353652,410919,89993,295390,102709,59503\n",
    ",130926,89899,144029,215227,73860,374624,308040,351509,139379,159354\n",
    ",364270,220763,145285,275267,329419,167087,371122,86523,341677,130926\n",
    ",139379,374624,45216,198282,243689,71124,30567,374624,194533,155740\n",
    ",109864,36328,301724,63803,112696,70655,223497,189093,139379,376285\n",
    ",109864,267661,374624,194533,325238,194533,125851,125851,345992,275267\n",
    ",313536,134427,260398,130926,295390,295390,128522,329419,128387,3407\n",
    ",47818,301724,388838,11160,304048,176043,413602,41105,174272,380477\n",
    ",190119,66629,240969,315735,16190,34991,396897,235970,295390,301724\n",
    ",159354,45013,60696,301724,49492,156529,280748,41105,295390,63803\n",
    ",104110,72820,349524,395469,302654,240969,87472,301724,295390,422419\n",
    ",236970,413602,81152,3407,294185,198282,269326,125376,235970,319605\n",
    ",152246,349524,281898,156529,24529,374624,280748,410734,301724,237570\n",
    ",122711,342186,394820,142540,80754,111480,93274,269326,368894,60696\n",
    ",236970,194533,342186,31064,45376,329419,89993,376285,166836,269326\n",
    ",348037,236970,301724,54423,3407,329419,116849,301724,220698,364510\n",
    ",255216,371122,145685,60976,301724,23617,304048,298098,337080,275267\n",
    ",48054,167758,172761,80754,424120,301927,374624,116849,245911,54423\n",
    ",73860,295390,3407,71711,2859,234829,169079,86523,424120,295390\n",
    ",81152,152442,134452,130926,383237,70655,315735,396897,367093,34991\n",
    ",99231,104540,351487,167007,284961,240969,244632,63739,152442,210004\n",
    ",356883,66629,37874,329419,380477,301724,172761,325238,291029,202337\n",
    ",321088,101168,260398,375861,199307,104110,376609,99231,325238,46067\n",
    ",128442,3407,81134,177017,80754,339137,45216,202337,128398,194533\n",
    ",340685,243689,144029,152442,112955,261495,333917,301724,67049,410928\n",
    ",175658,267661,198282,358948,333917,374624,3407,45376,76922,135763\n",
    ",130926,114197,58431,200295,348037,295390,223497,301724,325238,165031\n",
    ",194533,139379,206446,398356,301724,80754,198282,87472,329419,288151\n",
    ",261305,134427,194533,300212,49492,169032,86523,366897,71124,292091\n",
    ",160241,144150,126587,417858,167087,160241,93274,301294,187779,301724\n",
    ",301724,329419,136890,18084]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "l = np.concatenate([np.asarray(np.reshape(l, (-1, 1)), np.int32), np.random.rand(len(l), 901) ], 1)\n",
    "\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## required for semi-hard triplet loss:\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "import numpy as np\n",
    "\n",
    "def triplet_loss(vects):\n",
    "    margin = 1.\n",
    "    labels = vects[:, :1]\n",
    " \n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "\n",
    "    embeddings = tf.cast(vects[:, 1:], dtype='float32')\n",
    "\n",
    "    ### Code from Tensorflow function [tf.contrib.losses.metric_learning.triplet_semihard_loss] starts here:\n",
    "    \n",
    "    # Reshape [batch_size] label tensor to a [batch_size, 1] label tensor.\n",
    "    # lshape=array_ops.shape(labels)\n",
    "    # assert lshape.shape == 1\n",
    "    # labels = array_ops.reshape(labels, [lshape[0], 1])\n",
    "\n",
    "    # Build pairwise squared distance matrix.\n",
    "    pdist_matrix = pairwise_distance(embeddings, squared=True)\n",
    "    # Build pairwise binary adjacency matrix.\n",
    "    adjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
    "    # Invert so we can select negatives only.\n",
    "    adjacency_not = math_ops.logical_not(adjacency)\n",
    "\n",
    "    # global batch_size  \n",
    "    batch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
    "\n",
    "    # Compute the mask.\n",
    "    pdist_matrix_tile = array_ops.tile(pdist_matrix, [batch_size, 1])\n",
    "    mask = math_ops.logical_and(\n",
    "        array_ops.tile(adjacency_not, [batch_size, 1]),\n",
    "        math_ops.greater(\n",
    "            pdist_matrix_tile, array_ops.reshape(\n",
    "                array_ops.transpose(pdist_matrix), [-1, 1])))\n",
    "    mask_final = array_ops.reshape(\n",
    "        math_ops.greater(\n",
    "            math_ops.reduce_sum(\n",
    "                math_ops.cast(mask, dtype=dtypes.float32), 1, keepdims=True),\n",
    "            0.0), [batch_size, batch_size])\n",
    "    mask_final = array_ops.transpose(mask_final)\n",
    "\n",
    "    adjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
    "    mask = math_ops.cast(mask, dtype=dtypes.float32)\n",
    "\n",
    "    # negatives_outside: smallest D_an where D_an > D_ap.\n",
    "    negatives_outside = array_ops.reshape(\n",
    "        masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n",
    "    negatives_outside = array_ops.transpose(negatives_outside)\n",
    "\n",
    "    # negatives_inside: largest D_an.\n",
    "    negatives_inside = array_ops.tile(\n",
    "        masked_maximum(pdist_matrix, adjacency_not), [1, batch_size])\n",
    "    semi_hard_negatives = array_ops.where(\n",
    "        mask_final, negatives_outside, negatives_inside)\n",
    "\n",
    "    loss_mat = math_ops.add(margin, pdist_matrix - semi_hard_negatives)\n",
    "\n",
    "    mask_positives = math_ops.cast(\n",
    "        adjacency, dtype=dtypes.float32) - array_ops.diag(\n",
    "        array_ops.ones([batch_size]))\n",
    "\n",
    "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "    #   in semihard, they take all positive pairs except the diagonal.\n",
    "    num_positives = math_ops.reduce_sum(mask_positives)\n",
    "\n",
    "    semi_hard_triplet_loss_distance = math_ops.truediv(\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.maximum(\n",
    "                math_ops.multiply(loss_mat, mask_positives), 0.0)),\n",
    "        num_positives,\n",
    "        name='triplet_semihard_loss')\n",
    "    \n",
    "    ### Code from Tensorflow function semi-hard triplet loss ENDS here.\n",
    "    return semi_hard_triplet_loss_distance\n",
    "\n",
    "def quintet_loss(inputs):\n",
    "\n",
    "    margin = 1.\n",
    "    labels = inputs[:, :1]\n",
    "\n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "\n",
    "    embeddings =  tf.cast(inputs[:, 1:], dtype='float32')\n",
    "\n",
    "    # Build pairwise squared distance matrix.\n",
    "    pdist_matrix = pairwise_distance(embeddings, squared=True)\n",
    "    # Build pairwise binary adjacency matrix.\n",
    "    adjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
    "    # Invert so we can select negatives only.\n",
    "    adjacency_not = math_ops.logical_not(adjacency)\n",
    "\n",
    "    # global batch_size  \n",
    "    batch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
    "\n",
    "    adjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
    "\n",
    "    mask_positives = math_ops.cast(\n",
    "        adjacency, dtype=dtypes.float32) - array_ops.diag(array_ops.ones([batch_size]))\n",
    "\n",
    "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "    #   in semihard, they take all positive pairs except the diagonal.\n",
    "    num_positives = math_ops.reduce_sum(mask_positives)\n",
    "\n",
    "    mask_negatives = adjacency_not\n",
    "\n",
    "    # Include the anchor to positives\n",
    "    mask_positives_centroids = math_ops.cast(adjacency, dtype=dtypes.float32)\n",
    "\n",
    "#     return mask_positives\n",
    "\n",
    "    # pos \n",
    "    embed_pos = tf.matmul(mask_positives_centroids, embeddings)\n",
    "    num_of_pos = tf.reduce_sum(mask_positives_centroids, axis=1, keepdims=True)\n",
    "    centroid_embed_pos = tf.math.xdivy(embed_pos, num_of_pos)\n",
    "    labels_pos = tf.cast(labels, dtype=dtypes.float32)\n",
    "    # negs\n",
    "    embed_neg = tf.matmul(mask_negatives, embeddings)\n",
    "    num_of_neg = tf.reduce_sum(mask_negatives, axis=1, keepdims=True)\n",
    "    centroid_embed_neg = tf.math.xdivy(embed_neg, num_of_neg)\n",
    "\n",
    "#     return mask_positives_centroids\n",
    "    i = tf.constant(0)\n",
    "    batch_centroid_matrix = tf.Variable([])\n",
    "    batch_centroid_matrix_neg = tf.Variable([])\n",
    "    batch_centroid_matrix_all = tf.Variable([])\n",
    "    def iter_centroids(i, batch_centroid_matrix, batch_centroid_matrix_neg, batch_centroid_matrix_all):\n",
    "        # centroid pos\n",
    "        mask_positives_batch = tf.reshape(tf.gather(mask_positives, i), (-1, 1))\n",
    "        centroid_pos = tf.gather(centroid_embed_pos, i)\n",
    "        \n",
    "        centroid_embed = tf.repeat([centroid_pos], repeats=[batch_size], axis=0)\n",
    "        new_batch_centroid_pos = mask_positives_batch * centroid_embed\n",
    "        new_batch_embeddings = tf.cast(tf.logical_not(tf.cast(mask_positives_batch, 'bool')), 'float32') * embeddings \n",
    "        new_batch = tf.reduce_sum([new_batch_centroid_pos, new_batch_embeddings], axis=0, keepdims=True)[0]\n",
    "        \n",
    "        vects_new_batch = tf.concat([labels_pos, new_batch], axis=1)\n",
    "        TL_new_batch = triplet_loss(vects_new_batch)\n",
    "        batch_centroid_matrix = tf.concat([batch_centroid_matrix, [TL_new_batch]], axis=0) \n",
    "        \n",
    "        # centroid neg\n",
    "        centroid_neg = tf.gather(centroid_embed_neg, i)\n",
    "        mask_negatives_batch = tf.reshape(tf.gather(mask_negatives, i), (-1, 1))\n",
    "        \n",
    "        centroid_embed = tf.repeat([centroid_neg], repeats=[batch_size], axis=0)\n",
    "        new_batch_centroid_neg = mask_negatives_batch * centroid_embed\n",
    "        new_batch_embeddings = tf.cast(tf.logical_not(tf.cast(mask_negatives_batch, 'bool')), 'float32') * embeddings \n",
    "        new_batch = tf.reduce_sum([new_batch_centroid_neg, new_batch_embeddings], axis=0, keepdims=True)[0]\n",
    "        \n",
    "        vects_new_batch = tf.concat([labels_pos, new_batch], axis=1)\n",
    "        TL_new_batch = triplet_loss(vects_new_batch)\n",
    "        batch_centroid_matrix_neg = tf.concat([batch_centroid_matrix_neg, [TL_new_batch]], axis=0) \n",
    "        \n",
    "        # centroid pos and neg\n",
    "        new_batch_centroids = tf.reduce_sum([new_batch_centroid_pos, new_batch_centroid_neg], axis=0, keepdims=True)[0]\n",
    "        vects_new_batch_centroids = tf.concat([labels_pos, new_batch_centroids], axis=1)\n",
    "        TL_new_batch = triplet_loss(vects_new_batch_centroids)\n",
    "        batch_centroid_matrix_all = tf.concat([batch_centroid_matrix_all, [TL_new_batch]], axis=0) \n",
    "        \n",
    "        return [tf.add(i, 1), batch_centroid_matrix, batch_centroid_matrix_neg, batch_centroid_matrix_all]\n",
    "    _, batch_centroid_matrix, batch_centroid_matrix_neg, batch_centroid_matrix_all = tf.while_loop(lambda i, a, b, c: i<batch_size, \n",
    "                                        iter_centroids, \n",
    "                                        [i, batch_centroid_matrix, batch_centroid_matrix_neg, batch_centroid_matrix_all],\n",
    "                                       shape_invariants=[i.get_shape(),\n",
    "                                                   tf.TensorShape([None]), tf.TensorShape([None]), tf.TensorShape([None])])\n",
    "\n",
    "    TL_anchor_w = 1.0 # tf.random_uniform_initializer(minval=0.0, maxval=1.)(shape=[1])[0]\n",
    "    TL_pos_w = 1.0 # tf.random_uniform_initializer(minval=0.0, maxval=1.)(shape=[1])[0]\n",
    "    TL_neg_w = 1.0 # tf.random_uniform_initializer(minval=0.0, maxval=1.)(shape=[1])[0]\n",
    "    TL_centroid_w = 1.0 # tf.random_uniform_initializer(minval=0.0, maxval=1.)(shape=[1])[0]\n",
    "    \n",
    "#     tl_weights = tf.truediv(num_of_pos, tf.reduce_max(num_of_pos))\n",
    "#     tl_w = tl_weights # tf.random_uniform_initializer(minval=0.0, maxval=tl_weights)(shape=[1])\n",
    "#     TL_pos_weighted = tf.reshape(batch_centroid_matrix, (-1, 1)) * tl_w\n",
    "#     TL_pos = tf.truediv(tf.reduce_sum(TL_pos_weighted), tf.reduce_sum(tl_w))\n",
    "\n",
    "    TL = triplet_loss(inputs)\n",
    "    TL_pos = tf.reduce_mean(batch_centroid_matrix)\n",
    "    TL_neg = tf.reduce_mean(batch_centroid_matrix_neg) #triplet_loss(vects_neg)\n",
    "    TL_centroid = tf.reduce_mean(batch_centroid_matrix_all) # triplet_loss(vects_centroids)\n",
    "\n",
    "    sum_of_median = tf.reduce_sum([TL * TL_anchor_w, TL_pos * TL_pos_w, TL_neg * TL_neg_w, TL_centroid * TL_centroid_w]) # \n",
    "    sum_of_weigths = TL_anchor_w + TL_pos_w + TL_neg_w + TL_centroid_w\n",
    "    weigthed_median = tf.truediv(sum_of_median, sum_of_weigths)    \n",
    "    return tf.cast([weigthed_median, TL_anchor_w, TL_pos_w, TL_neg_w, \n",
    "                    TL_centroid_w, TL, TL_pos, TL_neg, TL_centroid], \n",
    "                   dtype=dtypes.float32)\n",
    "\n",
    "v = tf.constant([[1.0, 1.0, 2.0], [2.0, 3.0, 4.0], [1.0, 1.0, 2.0], \n",
    "                 [1.0, 1.0, 2.0], [3.0, 3.0, 4.0], [3.0, 1.0, 2.0]]) # [2, 3.0, 4.0] [1, 1.0, 2.0]\n",
    "tl_loss = quintet_loss(v)\n",
    "sess=tf.Session() \n",
    "sess.run(tl_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
